---
title: AI/ML Safety, Bias and Security 

description: |
 <p style="text-align:justify">
  Functionality, Fairness and Security Validation of AI/ML-based Systems. 
 </p>
people:
  - ezekiel
  - sai
  - mayukh

layout: project
last-updated: 2023-03-08
---
<p style="text-align:justify">

The aim of this project is to employ rigorous software 
engineering principles for designing robust decision 
making systems (e.g., robust and secure artificial intelligent 
and machine-learning systems). To this end, we focus on 
various desirable properties of decision making systems, 
including but not limited to security (e.g., resilience against 
adversarial and backdoor attacks), robustness and fairness 
(i.e., removing social discrimination). In this topic, we have 
made some of the pioneering contributions, specifically, to 
discover inherent bias in AI/ML software. In terms of practical 
impact, our proposed techniques for functional and fairness testing 
methodologies have discovered hundreds and thousands of bugs in 
well-established AI/ML models developed or used by popular software 
industries including but not limited to Google, Microsoft, IBM, 
Amazon and Airbnb. 
</p>


<h3>Representative Publications:</h3>

<p style="text-align:justify">
<a href="https://asset-group.github.io/papers/KBCon.pdf">Knowledge-based Consistency Testing of Large Language Models</a><br>
Sai Sathiesh Rajan, Ezekiel Soremekun, and Sudipta Chattopadhyay<br>
The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP) Findings, 2024.
</p>


<p style="text-align:justify">
<a href="https://asset-group.github.io/papers/ImgFairMain.pdf">Distribution-aware Fairness Test Generation</a><br>
Sai Sathiesh Rajan, Ezekiel Soremekun, Yves Le Traon, and Sudipta Chattopadhyay<br>
Journal of Systems and Software (JSS), 2024. Presented at 39th IEEE/ACM International Conference on Automated Software Engineering (ASE 2024).
</p>

<p style="text-align:justify">
<a href="https://asset-group.github.io/papers/AEGIS.pdf">Towards Backdoor Attacks and Defense in Robust Machine Learning Models</a><br>
Ezekiel Soremekun, Sakshi Udeshi, and Sudipta Chattopadhyay<br>
Elsevier Journal of Computers and Security, 2023
</p>

<p style="text-align:justify">
<a href="https://asset-group.github.io/papers/AequeVox.pdf">AequeVox: Automated Fairness Testing of Speech Recognition Systems</a><br>
Sai Sathiesh Rajan, Sakshi Udeshi, and Sudipta Chattopadhyay<br>
25th International Conference on Fundamental Approaches to Software Engineering (FASE), 2022
</p>

<p style="text-align:justify">
<a href="https://asset-group.github.io/papers/Astrea.pdf"> Astraea: Grammar-based fairness testing</a><br>
Ezekiel Soremekun, Sakshi Sunil Udeshi, and Sudipta Chattopadhyay<br>
IEEE Transactions on Software Engineering (TSE), 2022
</p>

<p style="text-align:justify">
<a href="http://doi.acm.org/10.1145/3238147.3238165">Automated directed fairness testing</a><br>
Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay<br>
Proceedings of the 33rd (ACM/IEEE) International Conference on Automated Software Engineering (ASE), 2018
</p>

<p style="text-align:justify">
<a href="https://arxiv.org/abs/1902.10027">Grammar Based Directed Testing of Machine Learning Systems</a><br>
Sakshi Udeshi and Sudipta Chattopadhyay<br>
IEEE Transactions on Software Engineering (TSE), 2020
</p>


<h3>Interdisciplinary Publications:</h3>

<p style="text-align:justify">
Accented DH: Assessing Fairness of Multilingual Speech Recognition Systems<br>
Setsuko Yokoyama, Sai Sathiesh Rajan and Sudipta Chattopadhyay<br>
Digital Humanities (DH), 2023
</p>

<p style="text-align:justify">
<b>Acknowledgement:</b> We are grateful to 
<a href="https://www.moe.gov.sg/">Ministry of Education, Singapore</a>, <a href="https://www.ocft.com.sg/">OneConnect Financial</a> and 
<a href="https://temasek-labs.sutd.edu.sg/">Temasek Labs</a> for generously supporting this project. 
</p>
